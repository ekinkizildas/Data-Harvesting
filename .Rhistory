library(scrapex)
library(dotenv)
library(httr)
library(jsonlite)
library(furrr)
library(httr)
library(jsonlite)
library(tidyverse)
library(furrr)
library(httr)
library(jsonlite)
library(tidyverse)
library(furrr)
library(httr)
library(jsonlite)
library(tidyverse)
library(furrr)
dotenv::load_dot_env()
knitr::opts_chunk$set(echo = TRUE)
library(dotenv)
load_dot_env("api_key_au.env")
api_key <- Sys.getenv("API_KEY")
knitr::opts_chunk$set(echo = TRUE)
usethis::edit_r_environ()
knitr::opts_chunk$set(echo = TRUE)
library(dotenv)
# ✅ Enable parallel processing (4 workers)
plan(multisession, workers = 4)
usethis::edit_r_environ()
api_key <- Sys.getenv("api_key")
usethis::edit_r_environ()
library(scrapex)
library(dotenv)
library(httr)
library(jsonlite)
library(furrr)
library(httr)
library(jsonlite)
library(tidyverse)
library(furrr)
library(httr)
library(jsonlite)
library(tidyverse)
library(furrr)
library(httr)
library(jsonlite)
library(tidyverse)
library(furrr)
usethis::edit_r_environ()
api_key <- Sys.getenv("api_key")
api_key <- Sys.getenv("api_key")
# ✅ Enable parallel processing (4 workers)
plan(multisession, workers = 4)
# ✅ Define time range
start_year <- 2000
end_year <- as.numeric(format(Sys.Date(), "%Y"))
# ✅ Define countries
countries <- c("US", "GB", "DE", "FR", "TR", "IN", "JP", "ES", "IT", "BR")
# ✅ Define selected languages
languages <- c("es", "fr", "de", "tr", "ja", "ru", "zh", "ko", "pt", "hi", "it")
### **STEP 1: Fetch Top 50 Movies Per Language (Faster)**
fetch_movies_by_language <- function(lang) {
movies_list <- list()
for (page in 1:2) {  # ✅ Limit to first 2 pages (~50 movies per language)
movie_url <- paste0("https://api.themoviedb.org/3/discover/movie?api_key=", api_key,
"&sort_by=popularity.desc",
"&primary_release_date.gte=", start_year, "-01-01",
"&primary_release_date.lte=", end_year, "-12-31",
"&vote_count.gte=500",
"&with_original_language=", lang,
"&page=", page)
response <- tryCatch({
GET(movie_url, timeout(10))
}, error = function(e) NULL)
Sys.sleep(runif(1, 1, 2))  # ✅ Shorter delay (1-2 sec)
if (!is.null(response) && status_code(response) == 200) {
movies <- fromJSON(content(response, "text", encoding = "UTF-8"))$results
if (!is.null(movies) && length(movies) > 0) {
movie_df <- tibble(
movie_id = movies$id,
title = movies$title,
language = lang,
popularity = movies$popularity,
genre_ids = sapply(movies$genre_ids, function(x) paste(x, collapse = ",")) # ✅ Store genre codes
)
movies_list <- append(movies_list, list(movie_df))
}
}
}
if (length(movies_list) > 0) {
return(bind_rows(movies_list))
} else {
return(NULL)
}
}
# ✅ Fetch movies in parallel
all_movies <- future_map_dfr(languages, fetch_movies_by_language, .progress = TRUE)
library(future)
# ✅ Enable parallel processing (4 workers)
plan(multisession, workers = 4)
# ✅ Define time range
start_year <- 2000
end_year <- as.numeric(format(Sys.Date(), "%Y"))
# ✅ Define countries
countries <- c("US", "GB", "DE", "FR", "TR", "IN", "JP", "ES", "IT", "BR")
# ✅ Define selected languages
languages <- c("es", "fr", "de", "tr", "ja", "ru", "zh", "ko", "pt", "hi", "it")
### **STEP 1: Fetch Top 50 Movies Per Language (Faster)**
fetch_movies_by_language <- function(lang) {
movies_list <- list()
for (page in 1:2) {  # ✅ Limit to first 2 pages (~50 movies per language)
movie_url <- paste0("https://api.themoviedb.org/3/discover/movie?api_key=", api_key,
"&sort_by=popularity.desc",
"&primary_release_date.gte=", start_year, "-01-01",
"&primary_release_date.lte=", end_year, "-12-31",
"&vote_count.gte=500",
"&with_original_language=", lang,
"&page=", page)
response <- tryCatch({
GET(movie_url, timeout(10))
}, error = function(e) NULL)
Sys.sleep(runif(1, 1, 2))  # ✅ Shorter delay (1-2 sec)
if (!is.null(response) && status_code(response) == 200) {
movies <- fromJSON(content(response, "text", encoding = "UTF-8"))$results
if (!is.null(movies) && length(movies) > 0) {
movie_df <- tibble(
movie_id = movies$id,
title = movies$title,
language = lang,
popularity = movies$popularity,
genre_ids = sapply(movies$genre_ids, function(x) paste(x, collapse = ",")) # ✅ Store genre codes
)
movies_list <- append(movies_list, list(movie_df))
}
}
}
if (length(movies_list) > 0) {
return(bind_rows(movies_list))
} else {
return(NULL)
}
}
# ✅ Fetch movies in parallel
all_movies <- future_map_dfr(languages, fetch_movies_by_language, .progress = TRUE)
library(scrapex)
library(dotenv)
library(httr)
library(jsonlite)
library(furrr)
library(httr)
library(jsonlite)
library(tidyverse)
library(furrr)
library(httr)
library(jsonlite)
library(tidyverse)
library(furrr)
library(httr)
library(jsonlite)
library(tidyverse)
library(furrr)
library(future)
#usethis::edit_r_environ()
api_key <- Sys.getenv("api_key")
# ✅ Enable parallel processing (4 workers)
plan(multisession, workers = 4)
# ✅ Define time range
start_year <- 2000
end_year <- as.numeric(format(Sys.Date(), "%Y"))
# ✅ Define countries
countries <- c("US", "GB", "DE", "FR", "TR", "IN", "JP", "ES", "IT", "BR")
# ✅ Define selected languages
languages <- c("es", "fr", "de", "tr", "ja", "ru", "zh", "ko", "pt", "hi", "it")
### **STEP 1: Fetch Top 50 Movies Per Language (Faster)**
fetch_movies_by_language <- function(lang) {
movies_list <- list()
for (page in 1:2) {  # ✅ Limit to first 2 pages (~50 movies per language)
movie_url <- paste0("https://api.themoviedb.org/3/discover/movie?api_key=", api_key,
"&sort_by=popularity.desc",
"&primary_release_date.gte=", start_year, "-01-01",
"&primary_release_date.lte=", end_year, "-12-31",
"&vote_count.gte=500",
"&with_original_language=", lang,
"&page=", page)
response <- tryCatch({
GET(movie_url, timeout(10))
}, error = function(e) NULL)
Sys.sleep(runif(1, 1, 2))  # ✅ Shorter delay (1-2 sec)
if (!is.null(response) && status_code(response) == 200) {
movies <- fromJSON(content(response, "text", encoding = "UTF-8"))$results
if (!is.null(movies) && length(movies) > 0) {
movie_df <- tibble(
movie_id = movies$id,
title = movies$title,
language = lang,
popularity = movies$popularity,
genre_ids = sapply(movies$genre_ids, function(x) paste(x, collapse = ",")) # ✅ Store genre codes
)
movies_list <- append(movies_list, list(movie_df))
}
}
}
if (length(movies_list) > 0) {
return(bind_rows(movies_list))
} else {
return(NULL)
}
}
# ✅ Fetch movies in parallel
all_movies <- future_map_dfr(languages, fetch_movies_by_language, .progress = TRUE)
knitr::opts_chunk$set(echo = TRUE)
write.csv(final_data, "final_data.csv")
library(tidyverse)
library(tidytext)
library(ggwordcloud)
library(dplyr)
library(ggplot2)
library(stopwords)
library(igraph)
library(ggraph)
library(scales)
library(networkD3)
library(stats)
library(highcharter)
library(tidyr)
library(RColorBrewer)
final_data <- read.csv("final_data.csv")
# 1. Number of Movies by Year
ggplot(final_data, aes(x = as.factor(release_year))) +
geom_bar(fill = "blue", alpha = 0.7) +
labs(title = "Number of Movies by Year", x = "Release Year", y = "Movie Count") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# 2. Movie Distribution by Country (Top 10 countries producing the most movies)
top_countries <-final_data %>%
count(country, sort = TRUE) %>%
top_n(10, n)
ggplot(top_countries, aes(x = reorder(country, n), y = n)) +
geom_bar(stat = "identity", fill = "purple", alpha = 0.7) +
coord_flip() +
labs(title = "Top 10 Countries Producing the Most Movies", x = "Country", y = "Movie Count") +
theme_minimal()
# 3. Popularity Distribution
ggplot(final_data, aes(x = popularity)) +
geom_histogram(bins = 20, fill = "purple", color = "black", alpha = 0.7) +
labs(title = "Popularity Distribution", x = "Popularity", y = "Movie Count") +
theme_minimal()
# 5. Language Distribution (Top 10 most used languages)
top_languages <- final_data%>%
count(language, sort = TRUE) %>%
top_n(10, n)
ggplot(top_languages, aes(x = reorder(language, n), y = n)) +
geom_bar(stat = "identity", fill = "purple", alpha = 0.7) +
coord_flip() +
labs(title = "Top 10 Most Used Languages", x = "Language", y = "Movie Count") +
theme_minimal()
# Calculate total awards
final_data <- final_data %>%
mutate(total_awards = academy_award + academy_nominated + palme_dor + golden_lion)
# Scatter plot
ggplot(final_data, aes(x = popularity, y = total_awards)) +
geom_point(alpha = 0.6, color = "purple") +
geom_smooth(method = "lm", color = "red", se = FALSE) +  # Regression line
labs(title = "Popularity vs. Awards", x = "Popularity", y = "Total Awards") +
theme_minimal()
word_data <- final_data %>%
filter(!is.na(title)) %>%
unnest_tokens(word, title) %>%
count(word, sort = TRUE) %>%
filter(n > 5)  # Keep words appearing more than 5 times
# Remove stop words (English and other common words)
word_data <- word_data %>%
filter(!word %in% stopwords("en"))  # Remove common English stop words
# Improved Word Cloud
ggplot(word_data, aes(label = word, size = n, color = n)) +
geom_text_wordcloud(area_corr = TRUE) +
scale_color_viridis_c() +
labs(title = "Most Common Words in Movie Titles ") +
theme_minimal()
df_pairs <- final_data %>%
select(title, country) %>%
group_by(title) %>%
summarise(country_pairs = if (n_distinct(country) > 1) {
list(combn(unique(country), 2, simplify = FALSE))
} else {
list(NULL)
}) %>%
unnest(country_pairs) %>%
filter(!is.null(country_pairs)) %>%
unnest_wider(country_pairs, names_sep = "_") %>%
rename(from = country_pairs_1, to = country_pairs_2) %>%
count(from, to, name = "weight")
country_graph <- graph_from_data_frame(df_pairs, directed = FALSE)
print(country_graph)
library(networkD3)
library(dplyr)
# ✅ Create Nodes DataFrame
nodes <- data.frame(name = unique(c(df_pairs$from, df_pairs$to)))
nodes$id <- 0:(nrow(nodes) - 1)
# ✅ Create Links DataFrame
links <- df_pairs %>%
left_join(nodes, by = c("from" = "name")) %>%
rename(source = id) %>%
left_join(nodes, by = c("to" = "name")) %>%
rename(target = id) %>%
mutate(value = weight)
# ✅ Define Country Colors (d3 color scale)
color_scale <- 'd3.scaleOrdinal()
.domain(["US", "FR", "DE", "GB", "ES", "IT", "JP", "BR", "TR", "IN"])
.range(["#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd",
"#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf"])'
# ✅ Create the Force Network
network <- forceNetwork(Links = as.data.frame(links),
Nodes = as.data.frame(nodes),
Source = "source",
Target = "target",
NodeID = "name",
Value = "value",
Group = "name",
opacity = 0.8,
zoom = TRUE,
fontSize = 14,
linkDistance = 150,
charge = -500,
bounded = TRUE,
width = 1000, height = 800,
colourScale = JS(color_scale))
# ✅ JavaScript for Adding Legend (Fixing the Syntax)
legend_script <- JS('
function addLegend(svg) {
var legendData = [
{name: "US", color: "#1f77b4"},
{name: "FR", color: "#ff7f0e"},
{name: "DE", color: "#2ca02c"},
{name: "GB", color: "#d62728"},
{name: "ES", color: "#9467bd"},
{name: "IT", color: "#8c564b"},
{name: "JP", color: "#e377c2"},
{name: "BR", color: "#7f7f7f"},
{name: "TR", color: "#bcbd22"},
{name: "IN", color: "#17becf"}
];
var legend = svg.append("g")
.attr("transform", "translate(20,20)");  // Legend position
legend.selectAll("rect")
.data(legendData)
.enter()
.append("rect")
.attr("x", 10)
.attr("y", function(d, i) { return i * 20; })
.attr("width", 15)
.attr("height", 15)
.style("fill", function(d) { return d.color; });
legend.selectAll("text")
.data(legendData)
.enter()
.append("text")
.attr("x", 30)
.attr("y", function(d, i) { return i * 20 + 12; })
.text(function(d) { return d.name; })
.style("font-size", "14px")
.attr("alignment-baseline", "middle");
}
')
# Attach Legend to Network
network$x$options$script <- legend_script
# ender the Network Graph with Legend
network
